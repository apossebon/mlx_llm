from mlx_lm import load, generate, stream_generate 
from mlx_lm.sample_utils import make_sampler, make_logits_processors
from mlx_lm.models.cache import make_prompt_cache
from datetime import datetime
import re
import json

from openai_harmony import (
    load_harmony_encoding,
    HarmonyEncodingName,
    Role,
    Message,
    Conversation,
    DeveloperContent,
    SystemContent,
    Author,
    TextContent,
)

from mlx_lm.server import run 

encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)


functions_definition_example = """
# Instru√ß√µes

Utilize as fun√ß√µes dispon√≠veis para fornecer respostas precisas e detalhadas.

# Ferramentas

namespace functions {
    type getDataHora = () => any;
} // namespace functions
"""

def getDataHora():
    return {
        "data_hora": datetime.now().strftime("%d/%m/%Y %H:%M:%S"),
        "timezone": "America/Sao_Paulo"
    }

# Dicion√°rio de fun√ß√µes dispon√≠veis
available_functions = {
    "getDataHora": getDataHora
}

def detect_function_call(text):
    """
    Detecta se o texto cont√©m uma chamada de fun√ß√£o no formato Harmony
    """
    # Padr√£o para detectar chamadas de fun√ß√£o no formato Harmony
    function_call_pattern = r'<\|call\|>commentary<\|message\|>.*?<\|end\|>'
    
    if re.search(function_call_pattern, text):
        return True
    
    # Tamb√©m verifica se h√° men√ß√£o a fun√ß√µes espec√≠ficas
    for func_name in available_functions.keys():
        if f"functions.{func_name}" in text or func_name in text:
            return True
    
    return False

def extract_function_call(text):
    """
    Extrai informa√ß√µes da chamada de fun√ß√£o do texto
    """
    # Procurar por padr√µes de chamada de fun√ß√£o
    for func_name in available_functions.keys():
        # Verificar diferentes padr√µes de chamada
        patterns = [
            f"functions.{func_name}",
            f"<|call|>commentary<|message|>.*?{func_name}",
            f"to=functions.{func_name}",
            func_name
        ]
        
        for pattern in patterns:
            if pattern in text:
                return {
                    "function_name": func_name,
                    "arguments": {}  # Por enquanto, sem argumentos
                }
    
    return None

def execute_function(function_name, arguments):
    """
    Executa a fun√ß√£o especificada com os argumentos fornecidos
    """
    if function_name in available_functions:
        try:
            result = available_functions[function_name](**arguments)
            return result
        except Exception as e:
            return {"error": f"Erro ao executar {function_name}: {str(e)}"}
    else:
        return {"error": f"Fun√ß√£o {function_name} n√£o encontrada"}

def create_harmony_tool_response(function_name, result):
    """
    Cria uma resposta de ferramenta no formato Harmony correto
    """
    result_json = json.dumps(result, ensure_ascii=False)
    
    # Formato correto para resposta de ferramenta no Harmony
    tool_response = f"<|channel|>commentary<|message|>{result_json}<|end|>"
    
    return tool_response

def create_harmony_conversation_with_tool_result(original_prompt, function_name, result, functions_definition):
    """
    Cria uma nova conversa Harmony incluindo o resultado da ferramenta
    """
    result_json = json.dumps(result, ensure_ascii=False)
    
    # Criar mensagens no formato Harmony
    system_message = Message.from_role_and_content(
        Role.SYSTEM,
        SystemContent.new().with_reasoning_effort("Low")
    )
    
    developer_message = Message.from_role_and_content(
        Role.DEVELOPER,
        DeveloperContent.new().with_instructions(functions_definition)
    )
    
    user_message = Message.from_role_and_content(
        Role.USER, 
        original_prompt
    )
    
    # Criar mensagem de resposta da ferramenta no formato correto
    # from openai_harmony import TextContent
    # tool_content = TextContent(text=f"name:{function_name}, content:{result_json}")
    # tool_message = Message.from_role_and_content(
    #     Role.TOOL,
    #     tool_content
    # )

    # Criar mensagem de ferramenta corretamente
    tool_message = Message.from_author_and_content(
        Author.new(Role.TOOL, function_name),  # Especifica qual ferramenta
        TextContent(text=result_json)
    ).with_channel("commentary")
    
    # Construir conversa Harmony com resultado da ferramenta
    conversation = Conversation.from_messages([
        system_message, 
        developer_message, 
        user_message,
        tool_message
    ])
    
    return conversation


def render_harmony_conversation(usermessage: str, functions_definition: str):
    # Criar mensagens no formato Harmony
        system_message = Message.from_role_and_content(
            Role.SYSTEM,
            SystemContent.new().with_reasoning_effort("Low")
        )
        
        developer_message = Message.from_role_and_content(
            Role.DEVELOPER,
            DeveloperContent.new().with_instructions(functions_definition)
        )
        
        user_message = Message.from_role_and_content(
            Role.USER, 
            usermessage
        )
        
        # Construir conversa Harmony
        conversation = Conversation.from_messages([
            system_message, 
            developer_message, 
            user_message
        ])
        
        # Renderizar para tokens
        tokens = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)
        print(f"Tokens Harmony gerados: {len(tokens)}")
        return tokens



def main():
    print("Hello from mlx-llm!")
    DEFAULT_MODEL_ID = "mlx-community/gpt-oss-20b-MXFP4-Q8"
    Qwen_MODEL_ID = "lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-4bit"
   
    model, tokenizer = load(DEFAULT_MODEL_ID)

    _mlx_sampler = make_sampler(temp=0.7, top_p=0.85, top_k=40)
    _mlx_logits_processors = make_logits_processors(repetition_penalty=1.15, repetition_context_size=50)
    _mlx_prompt_cache = make_prompt_cache(model)

    # prompt = "Conte uma hist√≥ria sobre a vida de Einstein"

    # messages = [{"role": "user", "content": prompt}]
   
    # prompt_tokenized = tokenizer.apply_chat_template(
    #     messages, add_generation_prompt=True
    # )

    prompt = "Qual √© a data e hora atual? Explique tamb√©m como o tempo funciona."


    while True:
        prompt = input("Digite sua pergunta: ")
        if prompt == "exit":
            break
        
        # Renderizar conversa Harmony inicial
        prompt_harmony = render_harmony_conversation(prompt, functions_definition_example)
        
        # Gerar resposta com intercepta√ß√£o de fun√ß√µes
        full_response = ""
        function_called = False
        function_result = None
        
        print("ü§ñ Resposta do LLM:")
        tokens_respose =[]
        for response in stream_generate(model, tokenizer, prompt_harmony, max_tokens=1024, sampler=_mlx_sampler, logits_processors=_mlx_logits_processors, prompt_cache=_mlx_prompt_cache):
            print(response.text, end="", flush=True)
            full_response += response.text
            tokens_respose.append(response.token)
            # print(response.token)


            
            # Verificar se h√° chamada de fun√ß√£o durante o streaming
            if detect_function_call(full_response) and not function_called:
                function_called = True
                print("\n\nüîß Detectada chamada de fun√ß√£o durante o streaming!")
                
                # Extrair informa√ß√µes da chamada
                function_call = extract_function_call(full_response)
                if function_call:
                    print(f"Executando fun√ß√£o: {function_call['function_name']}")
                    
                    # Executar fun√ß√£o
                    function_result = execute_function(function_call['function_name'], function_call['arguments'])
                    print(f"Resultado: {function_result}")
                    
                    # Parar o streaming atual
                    break
                else:
                    print("‚ö†Ô∏è N√£o foi poss√≠vel extrair informa√ß√µes da chamada de fun√ß√£o")
                    print(f"Texto detectado: {full_response[-200:]}")  # √öltimos 200 caracteres
        
        # Se uma fun√ß√£o foi chamada, continuar com o resultado
        if function_called and function_result:
            print("\n\nüîÑ Continuando com resultado da fun√ß√£o...")
            
            # Criar conversa Harmony com resultado da ferramenta
            conversation_with_result = create_harmony_conversation_with_tool_result(
                prompt, function_call['function_name'], function_result, functions_definition_example
            )
            
            # Renderizar para tokens
            result_tokens = encoding.render_conversation_for_completion(conversation_with_result, Role.ASSISTANT)
            print(f"Tokens Harmony com resultado: {len(result_tokens)}")
            
            # Gerar resposta final
            print("ü§ñ Resposta final:")
            for response in stream_generate(model, tokenizer, result_tokens, max_tokens=1024, sampler=_mlx_sampler, logits_processors=_mlx_logits_processors, prompt_cache=_mlx_prompt_cache):
                print(response.text, end="", flush=True)
        
        # print(full_response)
        print("\n\n" + "="*50 + "\n")




if __name__ == "__main__":
    main()
