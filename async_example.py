#!/usr/bin/env python3
"""
Exemplo PrÃ¡tico: Uso AssÃ­ncrono do ChatMLX
Demonstra como usar _agenerate para processamento concorrente
"""

import sys
import asyncio
import time
sys.path.append('src')

from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
from langchain.agents import create_agent
from src.chatmlx import ChatMLX
from src.message_utils import pretty_print_messages, print_conversation_summary


@tool
def get_weather(location: str) -> str:
    """ObtÃ©m informaÃ§Ãµes meteorolÃ³gicas detalhadas."""
    weather_data = {
        "SÃ£o Paulo": "ğŸŒ¤ï¸ Ensolarado, 25Â°C, umidade 60%",
        "Rio de Janeiro": "â›… Parcialmente nublado, 28Â°C, umidade 75%", 
        "London": "â˜ï¸ Nublado, 12Â°C, umidade 80%",
        "Tokyo": "â˜€ï¸ Ensolarado, 22Â°C, umidade 55%",
        "New York": "ğŸŒ§ï¸ Chuva leve, 15Â°C, umidade 90%"
    }
    
    for city, weather in weather_data.items():
        if location.lower() in city.lower() or city.lower() in location.lower():
            return f"Clima em {city}: {weather}"
    
    return f"ğŸŒ¤ï¸ Clima em {location}: Ensolarado, 22Â°C (simulado)"


@tool
def calculate(expression: str) -> str:
    """Realiza cÃ¡lculos matemÃ¡ticos."""
    try:
        result = eval(expression.replace("^", "**").replace(" ", ""))
        return f"ğŸ§® {expression} = {result}"
    except Exception as e:
        return f"âŒ Erro: {str(e)}"


@tool
def search_info(topic: str) -> str:
    """Busca informaÃ§Ãµes sobre um tÃ³pico."""
    info_db = {
        "python": "ğŸ Linguagem de programaÃ§Ã£o interpretada e de alto nÃ­vel",
        "ai": "ğŸ¤– InteligÃªncia Artificial - campo da ciÃªncia da computaÃ§Ã£o",
        "machine learning": "ğŸ“Š Subcampo da IA focado em aprendizado automÃ¡tico",
        "blockchain": "â›“ï¸ Tecnologia de registro distribuÃ­do e criptografado"
    }
    
    topic_lower = topic.lower()
    for key, info in info_db.items():
        if key in topic_lower:
            return f"ğŸ“š {key.title()}: {info}"
    
    return f"ğŸ” InformaÃ§Ãµes sobre '{topic}': TÃ³pico interessante para pesquisa!"


async def process_single_request(model, question: str, request_id: int):
    """Processa uma Ãºnica requisiÃ§Ã£o de forma assÃ­ncrona"""
    print(f"ğŸš€ Iniciando requisiÃ§Ã£o {request_id}: {question}")
    
    start_time = time.time()
    
    try:
        # Usar _agenerate diretamente
        messages = [HumanMessage(content=question)]
        result = await model._agenerate(messages)
        
        duration = time.time() - start_time
        
        content = result.generations[0].message.content
        tool_calls = result.generations[0].message.tool_calls or []
        
        print(f"âœ… RequisiÃ§Ã£o {request_id} concluÃ­da em {duration:.2f}s")
        print(f"   Resposta: {content[:80]}...")
        print(f"   Tool calls: {len(tool_calls)}")
        
        return {
            "id": request_id,
            "question": question,
            "result": result,
            "duration": duration,
            "success": True
        }
        
    except Exception as e:
        duration = time.time() - start_time
        print(f"âŒ RequisiÃ§Ã£o {request_id} falhou em {duration:.2f}s: {e}")
        
        return {
            "id": request_id,
            "question": question,
            "error": str(e),
            "duration": duration,
            "success": False
        }


async def concurrent_processing_demo():
    """DemonstraÃ§Ã£o de processamento concorrente"""
    print("ğŸ¯ DEMO: Processamento AssÃ­ncrono Concorrente")
    print("=" * 60)
    
    # Criar modelo com ferramentas
    model = ChatMLX()
    model.init()
    model_with_tools = model.bind_tools([get_weather, calculate, search_info])
    
    # Lista de perguntas para processar concorrentemente
    questions = [
        "What's the weather in SÃ£o Paulo?",
        "Calculate 15 * 23 + 100",
        "Tell me about machine learning",
        "What's the weather in London?", 
        "Calculate 50 / 2",
        "Search information about Python",
        "Weather in Tokyo?",
        "What is 2^8?"
    ]
    
    print(f"ğŸ“‹ Processando {len(questions)} perguntas concorrentemente...")
    print("â±ï¸ Iniciando cronÃ´metro...")
    
    start_total = time.time()
    
    # Criar tasks para todas as requisiÃ§Ãµes
    tasks = [
        process_single_request(model_with_tools, question, i+1) 
        for i, question in enumerate(questions)
    ]
    
    # Executar todas concorrentemente
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    total_duration = time.time() - start_total
    
    print(f"\nğŸ Processamento concorrente concluÃ­do!")
    print(f"â±ï¸ Tempo total: {total_duration:.2f}s")
    print(f"ğŸ“Š MÃ©dia por requisiÃ§Ã£o: {total_duration/len(questions):.2f}s")
    
    # Analisar resultados
    successful = [r for r in results if isinstance(r, dict) and r.get('success')]
    failed = [r for r in results if isinstance(r, dict) and not r.get('success')]
    exceptions = [r for r in results if isinstance(r, Exception)]
    
    print(f"\nğŸ“ˆ EstatÃ­sticas:")
    print(f"   âœ… Sucessos: {len(successful)}")
    print(f"   âŒ Falhas: {len(failed)}")
    print(f"   ğŸš« ExceÃ§Ãµes: {len(exceptions)}")
    
    if successful:
        avg_duration = sum(r['duration'] for r in successful) / len(successful)
        print(f"   â±ï¸ Tempo mÃ©dio individual: {avg_duration:.2f}s")
    
    # Mostrar alguns resultados
    print(f"\nğŸ“ Primeiros 3 resultados:")
    for i, result in enumerate(results[:3]):
        if isinstance(result, dict) and result.get('success'):
            print(f"\n{i+1}. {result['question']}")
            content = result['result'].generations[0].message.content
            print(f"   Resposta: {content[:100]}...")
            print(f"   Tempo: {result['duration']:.2f}s")


async def agent_async_demo():
    """DemonstraÃ§Ã£o com create_agent assÃ­ncrono"""
    print("\nğŸ¯ DEMO: create_agent com Suporte AssÃ­ncrono")
    print("=" * 60)
    
    # Criar agent
    model = ChatMLX()
    model.init()
    
    agent = create_agent(
        model=model,
        tools=[get_weather, calculate, search_info],
        prompt="VocÃª Ã© um assistente Ãºtil. Responda em portuguÃªs quando possÃ­vel."
    )
    
    # Perguntas para o agent
    agent_questions = [
        "Qual Ã© o clima em SÃ£o Paulo e calcule 10 * 5?",
        "Me fale sobre Python e qual Ã© o clima em Londres?",
        "Calcule 100 / 4 e busque informaÃ§Ãµes sobre AI"
    ]
    
    print(f"ğŸ¤– Testando {len(agent_questions)} perguntas com agent...")
    
    start_time = time.time()
    
    # Processar com agent (que pode usar _agenerate internamente)
    agent_tasks = []
    for i, question in enumerate(agent_questions):
        print(f"\nğŸ’¬ Pergunta {i+1}: {question}")
        
        # Agent invoke (sÃ­ncrono, mas o modelo interno pode usar async)
        result = agent.invoke({"messages": [HumanMessage(content=question)]})
        
        print("ğŸ“œ Resultado:")
        print_conversation_summary(result)
    
    agent_duration = time.time() - start_time
    print(f"\nâ±ï¸ Tempo total do agent: {agent_duration:.2f}s")


async def main():
    """FunÃ§Ã£o principal da demonstraÃ§Ã£o"""
    print("ğŸ¤– ChatMLX - DemonstraÃ§Ã£o de Funcionalidades AssÃ­ncronas")
    print("ğŸ“š Baseado na implementaÃ§Ã£o de _agenerate")
    print("=" * 70)
    
    try:
        # Demo 1: Processamento concorrente
        await concurrent_processing_demo()
        
        # Demo 2: Agent com suporte assÃ­ncrono
        await agent_async_demo()
        
        print("\nğŸ‰ TODAS AS DEMOS CONCLUÃDAS!")
        print("âœ… _agenerate implementado e funcionando")
        print("ğŸš€ Processamento concorrente ativo")
        print("âš¡ Performance melhorada com asyncio")
        print("ğŸ”§ CompatÃ­vel com create_agent do LangChain")
        
    except Exception as e:
        print(f"âŒ Erro durante as demos: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
